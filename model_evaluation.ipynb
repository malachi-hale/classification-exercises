{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b47b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae0b4a",
   "metadata": {},
   "source": [
    "## Given the following confusion matrix, evaluate (by hand) the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca4cea",
   "metadata": {},
   "source": [
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f6e42",
   "metadata": {},
   "source": [
    "#### In the context of this problem, what is a false positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94e189",
   "metadata": {},
   "source": [
    "A false positive would be that we predicted a dog, but there the observation was an actual cat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1968693",
   "metadata": {},
   "source": [
    "#### In the context of this problem, what is a false negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e881bdb",
   "metadata": {},
   "source": [
    "A false negative would be that we predicted a cat, but the observation was in fact a dog. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560217b2",
   "metadata": {},
   "source": [
    "#### How would you describe this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a21de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (46 + 34)/(46+7+13+34)\n",
    "precision = (46)/(46+13)\n",
    "recall = (46)/(46+7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf55a31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has an accuracy rate of 0.8 a precision rate of 0.78 and a recall rate of  0.87 .\n"
     ]
    }
   ],
   "source": [
    "print(\"The model has an accuracy rate of\", accuracy, \"a precision rate of\", round(precision, 2) , \"and a recall rate of \", round(recall, 2), \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5414b4",
   "metadata": {},
   "source": [
    "## Question Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be048db1",
   "metadata": {},
   "source": [
    "You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a5b4f",
   "metadata": {},
   "source": [
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions is found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd949679",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = attendance = pd.read_csv('c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf4190ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b36c85",
   "metadata": {},
   "source": [
    "### Use the predictions dataset and pandas to help answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce61c1",
   "metadata": {},
   "source": [
    "An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4fc4c",
   "metadata": {},
   "source": [
    "#### Which evaluation metric would be appropriate here? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825e83c",
   "metadata": {},
   "source": [
    "The recall metric is the best metric to use in this case because False Negatives are more damaging to company revenues than False Positives. False Positives will force us to throw out  good rubber ducks, which is certainly bad for the company. However, False Negatives will cause us to try to sell defective rubber ducks, which could have long term implications for our company, including damaging our comapany's reputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7139a9",
   "metadata": {},
   "source": [
    "#### Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44355249",
   "metadata": {},
   "source": [
    "We will evaluate the recall for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e65aa872",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = c3[c3.actual == 'Defect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e256f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_recall = (subset.model1 == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b67573dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_recall = (subset.model2 == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079cdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_recall = (subset.model3 == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7406e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 recall: 0.5\n",
      "Model 2 recall: 0.5625\n",
      "Model 3 recall: 0.8125\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1 recall:\", model1_recall)\n",
    "print(\"Model 2 recall:\", model2_recall)\n",
    "print(\"Model 3 recall:\", model3_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6fccb",
   "metadata": {},
   "source": [
    "Since Model 3 has the highest recall rate, **Model 3** is the best fit for this case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
